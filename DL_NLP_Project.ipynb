{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CSV code"
      ],
      "metadata": {
        "id": "mLq8s_RsBvHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3Gk9RduWBujn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cc881c-0f28-4cbe-dbf1-37877398162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qugdaubQVdef",
        "outputId": "60e0fb30-ee14-48c7-cab9-1c54f666f6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thedevastator/tinystories-narrative-classification\n",
        "!unzip tinystories-narrative-classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwQC9XbMVeom",
        "outputId": "37a97c30-f076-4bae-fb34-1f3f8b9b2027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thedevastator/tinystories-narrative-classification\n",
            "License(s): CC0-1.0\n",
            "Downloading tinystories-narrative-classification.zip to /content\n",
            " 97% 561M/576M [00:08<00:00, 77.2MB/s]\n",
            "100% 576M/576M [00:08<00:00, 72.3MB/s]\n",
            "Archive:  tinystories-narrative-classification.zip\n",
            "  inflating: train.csv               \n",
            "  inflating: validation.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv') #, encoding='ISO-8859-1') #, skiprows=303350, nrows=10)"
      ],
      "metadata": {
        "id": "na27HkPCU1MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text_data = df['text'].str.cat(sep=' ')\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text_no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text_no_punctuation.lower())\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess_text(text_data)"
      ],
      "metadata": {
        "id": "87twj2czUyDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Markov\n",
        "class MarkovChain:\n",
        "    def __init__(self, order=2):\n",
        "        self.order = order\n",
        "        self.transitions = defaultdict(list)\n",
        "        self.start_states = []\n",
        "\n",
        "    def train(self, tokens):\n",
        "        for i in range(len(tokens) - self.order):\n",
        "            state = tuple(tokens[i:i+self.order])\n",
        "            next_token = tokens[i+self.order]\n",
        "            self.transitions[state].append(next_token)\n",
        "            if i == 0:\n",
        "                self.start_states.append(state)\n",
        "\n",
        "    def generate_sentence(self, max_length=20):\n",
        "        state = random.choice(self.start_states)\n",
        "        sentence = list(state)\n",
        "        for _ in range(max_length - self.order):\n",
        "            next_words = self.transitions.get(state)\n",
        "            if not next_words:\n",
        "                break\n",
        "            next_word = random.choice(next_words)\n",
        "            sentence.append(next_word)\n",
        "            state = tuple(sentence[-self.order:])\n",
        "        return ' '.join(sentence).capitalize() + '.'"
      ],
      "metadata": {
        "id": "cDU_-SuQNDih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Markov\n",
        "markov = MarkovChain(order=2)\n",
        "markov.train(tokens)"
      ],
      "metadata": {
        "id": "El-8J4fiNJJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataprep LSTM\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "max_sequence_len = max([len(word_tokenize(sent)) for sent in sent_tokenize(text_data)])"
      ],
      "metadata": {
        "id": "A4wGk5zb3Jop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# LSTM\n",
        "def build_lstm_model(vocab_size, max_sequence_len):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, 100, input_length=max_sequence_len-1))\n",
        "    model.add(tf.keras.layers.LSTM(150, return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(100))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def prepare_sequences(text, tokenizer, max_sequence_len, batch_size=1000):\n",
        "    sequences = []\n",
        "    lines = text.split('.')\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip():\n",
        "            token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "            for i in range(1, len(token_list)):\n",
        "                seq = token_list[:i + 1]\n",
        "                sequences.append(seq)\n",
        "\n",
        "            if len(sequences) >= batch_size:\n",
        "                sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "                yield sequences[:, :-1], sequences[:, -1]\n",
        "\n",
        "    if sequences:\n",
        "        sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
        "        yield sequences[:, :-1], sequences[:, -1]\n",
        "\n",
        "def create_dataset(text_data, tokenizer, max_sequence_len, batch_size=1000):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: prepare_sequences(text_data, tokenizer, max_sequence_len, batch_size),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, max_sequence_len-1), dtype=tf.int32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "dataset = create_dataset(text_data, tokenizer, max_sequence_len)\n",
        "model = build_lstm_model(total_words, max_sequence_len)\n",
        "model.fit(dataset, epochs=10, verbose=1)"
      ],
      "metadata": {
        "id": "mWA8BFbp6gTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e8a03a9-0d92-456c-8088-cc7cec70bca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 78ms/step - accuracy: 0.0978 - loss: 5.8366\n",
            "Epoch 2/10\n",
            "\u001b[1m   1/3370\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:26\u001b[0m 168ms/step - accuracy: 0.2808 - loss: 4.1591"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 80ms/step - accuracy: 0.2513 - loss: 4.2976\n",
            "Epoch 3/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 80ms/step - accuracy: 0.2779 - loss: 3.9784\n",
            "Epoch 4/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 79ms/step - accuracy: 0.2928 - loss: 3.8172\n",
            "Epoch 5/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 80ms/step - accuracy: 0.3021 - loss: 3.7129\n",
            "Epoch 6/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 80ms/step - accuracy: 0.3091 - loss: 3.6373\n",
            "Epoch 7/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 80ms/step - accuracy: 0.3148 - loss: 3.5793\n",
            "Epoch 8/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 80ms/step - accuracy: 0.3192 - loss: 3.5344\n",
            "Epoch 9/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 79ms/step - accuracy: 0.3233 - loss: 3.4935\n",
            "Epoch 10/10\n",
            "\u001b[1m3370/3370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 79ms/step - accuracy: 0.3269 - loss: 3.4591\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bb2b89d1cc0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "markov_chain = MarkovChain()\n",
        "lstm_model = build_lstm_model(total_words, max_sequence_len)\n",
        "tokenizer = Tokenizer()\n",
        "max_sequence_len = 20\n",
        "num_sentences = 10\n",
        "temperature = 1.0\n",
        "\n",
        "def generate_seeded_story(seed_word, markov_chain, lstm_model, tokenizer, max_sequence_len, num_sentences=10, next_words=5, temperature=1.0):\n",
        "    story = [seed_word]\n",
        "    initial_sentence = seed_word\n",
        "\n",
        "    for i in range(num_sentences - 1):\n",
        "        if i % 2 == 0:\n",
        "            for _ in range(next_words):\n",
        "                token_list = tokenizer.texts_to_sequences([initial_sentence])[0]\n",
        "                token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "                predicted = lstm_model.predict(token_list, verbose=0)\n",
        "                predicted_word_index = np.argmax(predicted)\n",
        "                output_word = tokenizer.index_word.get(predicted_word_index, \"<OOV>\")\n",
        "                initial_sentence += \" \" + output_word\n",
        "            story.append(initial_sentence)\n",
        "        else:\n",
        "            tokens = word_tokenize(initial_sentence.lower())\n",
        "            state = tuple(tokens[-markov_chain.order:])\n",
        "            next_word = random.choice(markov_chain.transitions.get(state, [\"\"]))\n",
        "            initial_sentence += \" \" + next_word\n",
        "            story.append(initial_sentence)\n",
        "\n",
        "    return ' '.join(story)\n",
        "\n",
        "seed_word = \"Once\"\n",
        "generated_seeded_story = generate_seeded_story(seed_word, markov, model, tokenizer, max_sequence_len, num_sentences=10, next_words=5)\n",
        "print(generated_seeded_story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt907pvEW20H",
        "outputId": "ca76420a-3f7f-47d7-80c3-a8f40d1a0840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spot . she had ruined it . lily enjoyed their pot of rice filled with ice cream and yummy food.\n"
          ]
        }
      ]
    }
  ]
}